\documentclass[utf8]{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{verbatim}

\usepackage{algorithm}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

\onehalfspacing

\title{\bf\huge Lab3 Design Document}
\author{Mei Yixuan 2019011041 Yao92}
\date{\today}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}  
\renewcommand{\algorithmicensure}{\textbf{Output:}} 

\begin{document}
\maketitle

\section{Advanced Cache Replacement Policies}

\subsection{LRU-LIP}
In LRU-LIP, we set counter of the newly inserted block as the number of valid ways in the corresponding set minus one. This ensures that newly added blocks are in the least important position. Also, the counters are continuous (i.e. if we have 3 valids ways, their counters have value 0, 1 and 2). This nice property makes eviction and reversion much easier: we can simply use the same function as in LRU. The hardware control overhead of LRU-LIP is one counter each way, which is identical to LRU. Exact hardware cost is in the following figure.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{screenshot001}
	\caption{LRU-LIP Hardware Cost}
	\label{fig:screenshot001}
\end{figure}


\subsection{DIP}
In DIP, we use the first set (SET0) as MIP sample and the second set (SET1) as LIP sample. In each memory access on non-dueling sets (i.e. sets other than SET0 and SET1), we update cache control fields according to current better replacement policy. We reset all counters every 100000 memory accesses to avoid potential risk of overflow. Thanks to the good property of LRU-LIP, eviction and reversion of DIP is also identical to LRU. Besides the counter in each way, DIP also needs five counters for data recording. Exact hardware cost is in the following figure.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{screenshot002}
	\caption{DIP Hardware Cost}
	\label{fig:screenshot002}
\end{figure}
\newpage

\subsection{RRIP}
In RRIP, we use counter field of each way to store its RRI. It has field width of 3 bits. Upon hit, we set RRI of corresponding entry as 0. Upon insertion, we set RRI of corresponding entry as long RRI (i.e. 6). When choosing a block for eviction when all ways are occupied, we choose the block with largest RRI and normalize all values to distant RRI (this is identical to adding one repeatedly). Exact hardware cost is in the following figure.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{screenshot003}
	\caption{RRIP Hardware Cost}
	\label{fig:screenshot003}
\end{figure}

\subsection{Results}
Results of different replacement policies:
\newline

\resizebox{\textwidth}{18mm}{
\begin{tabular}{|c|c|c|c|c|}
		\hline
		Design & Benchmark 1 Miss Rate & Benchmark 2 Miss Rate & Benchmark 1 Total Cycles & Benchmark 2 Total Cycles \\
		\hline
		No Cache & 100 & 100 & 2083191 & 23508468 \\
		\hline
		Random Replacement & 16.47 & 2.44 & 1230758 & 9275269 \\
		\hline
		LRU Replacement & 13.86 & 1.99 & 1204202 & 9209763 \\
		\hline
		LRU-LIP Replacement & 14.77 & 2.76 & 1213470 & 9322490 \\
		\hline
		DIP-Replacement & 14.66 & 2.27 & 1212268 & 9250427 \\
		\hline
		RRIP-Replacement & 13.71 & 2.24 & 1202611 & 9246016 \\
		\hline
\end{tabular}}
\newline

\noindent
Results of different cache capacities and mapping schemes:
\newline

\resizebox{\textwidth}{33mm}{
\begin{tabular}{|c|c|c|c|c|}
	\hline
	Capacity and Mapping & Benchmark 1 Miss Rate & Benchmark 2 Miss Rate & Benchmark 1 Total Cycles & Benchmark 2 Total Cycles \\
	\hline
	16-entry direct-mapped & 29.71 & 26.23 & 1365868 & 12745909 \\
	\hline
	16-entry 4-way associative & 28.74 & 7.52 & 1356019 & 10016283 \\
	\hline
	16-entry 8-way associative & 29.67 & 3.56 & 1365525 & 9439026 \\
	\hline
	16-entry fully-associative & 30.32 & 3.53 & 1372080 & 9433830 \\
	\hline
	32-entry direct-mapped & 19.01 & 19.73 & 1256732 & 11798043 \\
	\hline
	32-entry 4-way associative & 13.86 & 1.99 & 1204202 & 9209763 \\
	\hline
	32-entry 8-way associative & 12.72 & 1.92 & 1192565 & 9199480 \\
	\hline
	32-entry fully-associative & 12.67 & 1.90 & 1191977 & 9196845 \\
	\hline
	64-entry direct-mapped & 13.80 & 6.51 & 1203530 & 9868691 \\
	\hline
	64-entry 4-way associative & 6.49 & 0.54 & 1128914 & 8997426 \\
	\hline
	64-entry 8-way associative & 6.24 & 0.43 & 1126334 & 8981639 \\
	\hline
	64-entry fully-associative & 6.18 & 0.39 & 1125792 & 8975471 \\
	\hline
\end{tabular}
}
\newpage

\section{Skewed-Associative Cache}

\subsection{Implementation}
In skewed-associative cache, we need separate hash functions for each way. Since we want blocks to be independently and uniformly mapped in each way, we choose a group of universal hash functions for mapping. Specifically, we choose $h_{ab}(k) = (a * k + b) \mod m$, in which $k$ is the block address, $m$ is the number of sets. In order for this group of hash functions to be universal, we need to choose $a$ and $b$ uniformly random for each way. Considering actual performance, we choose $a = 1$ for all ways, and $b$ be a random number for each way. Note that the tag function used for non-skewed cache is not suitable for skewed cache, since multiple blocks with the same tag may be mapped into the same way of the same set. Therefore, we use the whole address field as tag instead.

\subsection{Results}
\resizebox{\textwidth}{13mm}{
\begin{tabular}{|c|c|c|c|c|}
	\hline
	Capacity and Mapping & Benchmark 1 Miss Rate & Benchmark 2 Miss Rate & Benchmark 1 Total Cycles & Benchmark 2 Total Cycles \\
	\hline
	16-entry 4-way associative & 28.74 & 7.52 & 1356019 & 10016283 \\
	\hline
	16-entry 4-way skewed-associative & 28.53 & 8.92 & 1353896 & 10220843 \\
	\hline
	32-entry 4-way associative & 13.86 & 1.99 & 1204202 & 9209763 \\
	\hline
	32-entry 4-way skewed-associative & 13.98 & 2.02 & 1205348 & 9214083 \\
	\hline
\end{tabular}
}

\section{Benchmarks}

\subsection{Replacement Benchmarks}
\begin{figure}[h]
	\centering
	\subfigure[LRU] {\includegraphics[width=8cm, height=6cm]{screenshot004}}
	\subfigure[LRU-LIP]{\includegraphics[width=8cm, height=6cm]{screenshot005}}
	\caption{Replacement Benchmarks}
	\label{fig:screenshot004}
\end{figure}

The first program will have better performance on LRU. In this example, LRU will keep a[96] and a[128] in cache, while LRU-LIP will evict them alternately (note that all five elements are hashed into the same set). Therefore, LRU very low miss rate while LRU-LIP have 100\% miss rate. In the second example, LRU-LIP has better performance. In this example, LRU will cyclically evict blocks from cache, causing all misses. LRU, on the contrary, only incurs two misses each iteration. Therefore, it has much lower miss rate. In actual design of a processor, we may prefer LRU if the programs running on top of it has very good temperal locality, i.e. elements visited will soon be revisited again. If considerable amounts of memory accesses are ramdom access that won't repeat itself, LRU-LIP may be a better choice.

\newpage
\resizebox{\textwidth}{10mm}{
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Replacement Policy & bench\_lru Miss Rate & bench\_lrulip Miss Rate & bench\_lru Total Cycles & bench\_lrulip Total Cycles \\
		\hline
		LRU & 0.25 & 100 & 10055 & 59006 \\
		\hline
		LRU-LIP & 100 & 40.06 & 26039 & 35030 \\
		\hline
	\end{tabular}
}


\subsection{Writehit Benchmarks}
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm, height=6cm]{screenshot006}
	\caption{Writehit Benchmarks: Write-back}
	\label{fig:screenshot006}
\end{figure}


\resizebox{\textwidth}{10mm}{
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Write Hit Policy & bench\_through Miss Rate & bench\_back Miss Rate & bench\_through Total Cycles & bench\_back Total Cycles \\
		\hline
		Write-back & N/A & 0.25 & N/A & 10055 \\
		\hline
		Write-through & N/A & 0.25 & N/A & 26039 \\
		\hline
	\end{tabular}
}

\subsection{Writemiss Benchmarks}

\section{Ripes Bug Report}

\section{Question Answering}



\end{document}